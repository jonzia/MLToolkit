{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Machine Learning Toolkit\n",
    "Jonathan Zia (zia@gatech.edu)\n",
    "\n",
    "## Import Databases for Regression and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diabetes dataset (regression)\n",
    "from sklearn.datasets import load_diabetes\n",
    "# \"Digits\" dataset (classification)\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"digits\" dataset is composed of 1797 samples of 8x8 images. This results in a feature vector of length N = 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load \"digits\" dataset\n",
    "n_classes = 10 # Set number of classes of digits\n",
    "digits = load_digits(n_class = n_classes)\n",
    "\n",
    "# Print the dimensionality of the data\n",
    "print(digits.data.shape)\n",
    "print(digits.target.shape)\n",
    "\n",
    "# Write the data and targets to easier variables\n",
    "x, y = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example digit from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an example of the dataset\n",
    "import matplotlib.pyplot as plt \n",
    "plt.figure()\n",
    "plt.gray() \n",
    "plt.matshow(digits.images[0]) # <-- Integer number to view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "We can perform linear dimensionality reduction with PCA using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the decomposition sub-package\n",
    "from sklearn import decomposition\n",
    "\n",
    "# Initialize our PCA mapping\n",
    "# We set n_components < x.shape[1] to perform dim. reduction via elimination\n",
    "pca = decomposition.PCA(n_components = x.shape[1])\n",
    "\n",
    "# Fit the PCA mapping to the data\n",
    "pca.fit(x)\n",
    "\n",
    "# Transform the data using the learned mapping\n",
    "x_pca = pca.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the first three PCs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Matplotlib 3D utility\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "# Import color map utilities\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "\n",
    "# Get the first three dimensions of the new PCA data\n",
    "pca_data = x_pca[:, 0:3]\n",
    "\n",
    "# Initialize the figure\n",
    "fig, ax = plt.figure(), plt.axes(projection='3d')\n",
    "\n",
    "# Prepare the color map\n",
    "colormap = plt.get_cmap('gist_rainbow') # Import color map\n",
    "cNorm  = colors.Normalize(vmin=0, vmax=n_classes) # Normalize color map values to the number of classes\n",
    "scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=colormap) # Create a scalar map of the color map\n",
    "\n",
    "# Plot each point according to its class\n",
    "for i in range(0, x_pca.shape[0]):\n",
    "    # Get the color value corresponding to the current class\n",
    "    colorVal = scalarMap.to_rgba(y[i])\n",
    "    # Plot a scatterpoint with the proper color\n",
    "    ax.scatter3D(x_pca[i, 0], x_pca[i, 1], x_pca[i, 2], color = colorVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most useful attributes are the principal component (PC) vectors and variance explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PCs are an attribute of the trained PCA object\n",
    "PC = pca.components_\n",
    "\n",
    "# The variance explained is also an attribute of the object\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "\n",
    "# Plot the variance explained as a bar graph\n",
    "fig, ax = plt.figure(), plt.axes()\n",
    "ax.set_xlabel(\"Principal Component (PC)\")\n",
    "ax.set_ylabel(\"Variance Explained (%)\")\n",
    "ax.bar(range(0, pca.components_.shape[0]), var_exp)\n",
    "plt.title(\"Variance Explained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize clustering in the data with t-SNE using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "\n",
    "# Again, we start by initializing our t-SNE object\n",
    "# The attribute n_components sets the number of dimensions of the result\n",
    "# The perplexity is the key hyperparameter for t-SNE -> akin to # of nearest neighbors\n",
    "tsne = manifold.TSNE(n_components = 2, perplexity = 30)\n",
    "\n",
    "# Obtain the embedding of the data\n",
    "tsne_embedding = tsne.fit_transform(x)\n",
    "\n",
    "# Let's plot the embedding on a scatter plot\n",
    "fig, ax = plt.figure(), plt.axes()\n",
    "for i in range(0, y.shape[0]):\n",
    "    # Get the color associated with the class\n",
    "    colorVal = scalarMap.to_rgba(y[i])\n",
    "    # Plot the scatter point as defined by the t-SNE embedding\n",
    "    ax.scatter(tsne_embedding[i, 0], tsne_embedding[i, 1], c = colorVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "We'll explore how to implement various different classifiers with scikit-learn. Let's first partition the data into training and testing sets.\n",
    "\n",
    "### Training / Testing Set Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the samples are already randomized, we just split the set in half\n",
    "x_train, x_test = x[0:round(x.shape[0]/2), :], x[round(x.shape[0]/2) + 1:-1, :]\n",
    "y_train, y_test = y[0:round(y.shape[0]/2)], y[round(y.shape[0]/2) + 1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr_model = LogisticRegression(random_state = 42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lr_model.fit(x_train, y_train)\n",
    "\n",
    "# Generate predictions for testing data (class labels)\n",
    "preds_lr = lr_model.predict(x_test)\n",
    "\n",
    "# Let's visualize the performance with a confusion matrix!\n",
    "# Just importing some useful visualization packages here\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sb\n",
    "# Generating a confusion matrix\n",
    "cm = confusion_matrix(y_test, preds_lr)\n",
    "# It looks great in a gray colormap\n",
    "gray_map = plt.get_cmap('gist_yarg')\n",
    "# Converting the matrix to a heatmap for visualization\n",
    "heat_map = sb.heatmap(cm, cmap=gray_map)\n",
    "plt.title(\"Logistic Regression Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"LR Accuracy: \" + str(round(100*np.sum(preds_lr == y_test)/y_test.shape[0], 2)) + \"%\")\n",
    "\n",
    "# Let's visualize how confident we were in our predictions\n",
    "counter = np.zeros([1, n_classes]) # Total number of samples for each class\n",
    "cumulative = np.zeros([1, n_classes]) # Cumulative confidence over all samples of a particular class\n",
    "for i in range(0, x_test.shape[0]):\n",
    "    # Increment the counter for the current class\n",
    "    counter[0, y_test[i]] += 1\n",
    "    # Get the probability of the correct class for the current sample...\n",
    "    temp = lr_model.predict_proba(x_test[i, :].reshape(1, -1))\n",
    "    # ... and add it to the cumulative\n",
    "    cumulative[0, y_test[i]] += temp[0][y_test[i]]\n",
    "# Compute the confidence per sample in each class\n",
    "confidence = np.divide(cumulative, counter)\n",
    "# Plot the confidence on a bar graph, showing confidence in each class\n",
    "plt.bar(range(0, confidence.shape[1]), np.squeeze(confidence))\n",
    "plt.title(\"Logistic Regression Confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear/Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "# Initialize the LDA/QDA models\n",
    "lda, qda = LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis()\n",
    "\n",
    "# Generate fits using the training data\n",
    "lda.fit(x_train, y_train)\n",
    "qda.fit(x_train, y_train)\n",
    "\n",
    "# Generate predictions using the testing data\n",
    "preds_lda, preds_qda = lda.predict(x_test), qda.predict(x_test)\n",
    "\n",
    "# Print the accuracy of each model\n",
    "print(\"LDA Accuracy: \" + str(round(100*np.sum(preds_lda == y_test)/y_test.shape[0], 2)) + \"%\")\n",
    "print(\"QDA Accuracy: \" + str(round(100*np.sum(preds_qda == y_test)/y_test.shape[0], 2)) + \"%\")\n",
    "\n",
    "# Visualize the performance of each with a confusion matrix\n",
    "cm_lda, cm_qda = confusion_matrix(y_test, preds_lda), confusion_matrix(y_test, preds_qda)\n",
    "heat_map_lda = sb.heatmap(cm_lda, cmap=gray_map)\n",
    "plt.title(\"LDA Confusion Matrix\")\n",
    "plt.show()\n",
    "heat_map_qda = sb.heatmap(cm_qda, cmap=gray_map)\n",
    "plt.title(\"QDA Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Let's visualize how confident we were in our predictions\n",
    "counter = np.zeros([1, 10])\n",
    "lda_cumulative, qda_cumulative = np.zeros([1, 10]), np.zeros([1, 10])\n",
    "for i in range(0, x_test.shape[0]):\n",
    "    counter[0, y_test[i]] += 1\n",
    "    temp_lda = lda.predict_proba(x_test[i, :].reshape(1, -1))\n",
    "    temp_qda = qda.predict_proba(x_test[i, :].reshape(1, -1))\n",
    "    lda_cumulative[0, y_test[i]] += temp_lda[0][y_test[i]]\n",
    "    qda_cumulative[0, y_test[i]] += temp_qda[0][y_test[i]]\n",
    "lda_confidence, qda_confidence = np.divide(lda_cumulative, counter), np.divide(qda_cumulative, counter)\n",
    "plt.bar(range(0, lda_confidence.shape[1]), np.squeeze(lda_confidence))\n",
    "plt.title(\"LDA Confidence\")\n",
    "plt.show()\n",
    "plt.bar(range(0, qda_confidence.shape[1]), np.squeeze(qda_confidence))\n",
    "plt.title(\"QDA Confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize k-NN model and specify number of nearest neighbors\n",
    "neighbors = 5\n",
    "knn = KNeighborsClassifier(n_neighbors = neighbors)\n",
    "\n",
    "# Fit the model to the training data\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "# Test the model on the testing data\n",
    "preds_knn = knn.predict(x_test)\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print(\"k-NN Accuracy: \" + str(round(100*np.sum(preds_knn == y_test)/y_test.shape[0], 2)) + \"%\")\n",
    "\n",
    "# Visualize the performance with a confusion matrix\n",
    "cm_knn = confusion_matrix(y_test, preds_knn)\n",
    "heat_map_knn = sb.heatmap(cm_knn, cmap=gray_map)\n",
    "plt.title(\"k-NN Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Let's visualize how confident we were in our predictions\n",
    "counter, cumulative = np.zeros([1, 10]), np.zeros([1, 10])\n",
    "for i in range(0, x_test.shape[0]):\n",
    "    counter[0, y_test[i]] += 1\n",
    "    temp = knn.predict_proba(x_test[i, :].reshape(1, -1))\n",
    "    cumulative[0, y_test[i]] += temp[0][y_test[i]]\n",
    "confidence = np.divide(cumulative, counter)\n",
    "plt.bar(range(0, confidence.shape[1]), np.squeeze(confidence))\n",
    "plt.title(\"k-NN Confidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the random forest classifier\n",
    "# n_estimators: number of trees in the forest\n",
    "# max_depth: maximum depth of tree\n",
    "rf = RandomForestClassifier(n_estimators = 100, max_depth = 2, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "# Test the model on the testing data\n",
    "preds_rf = rf.predict(x_test)\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print(\"RF Accuracy: \" + str(round(100*np.sum(preds_rf == y_test)/y_test.shape[0], 2)) + \"%\")\n",
    "\n",
    "# Visualize the performance with a confusion matrix\n",
    "cm_rf = confusion_matrix(y_test, preds_rf)\n",
    "heat_map_rf = sb.heatmap(cm_rf, cmap=gray_map)\n",
    "plt.title(\"Random Forest Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Let's plot the feature importance!\n",
    "fig, ax = plt.figure(), plt.axes()\n",
    "ax.set_xlabel(\"Feature\")\n",
    "ax.set_ylabel(\"Importance\")\n",
    "plt.title(\"Random Forest Feature Importance\")\n",
    "rf_importance = ax.bar(range(0, 64), rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We'll test out polynomial fit functions on the \"diabetes\" dataset. First we'll load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load \"diabetes\" dataset\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "# Print the dimensionality of the data\n",
    "print(diabetes.data.shape)\n",
    "print(diabetes.target.shape)\n",
    "\n",
    "# Write the data and targets to easier variables\n",
    "x, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "x_train, x_test = x[0:round(x.shape[0]/2), :], x[round(x.shape[0]/2) + 1:-1, :]\n",
    "y_train, y_test = y[0:round(y.shape[0]/2)], y[round(y.shape[0]/2) + 1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Initialize the linear model\n",
    "linear_reg = linear_model.LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "linear_reg.fit(x_train, y_train)\n",
    "\n",
    "# Generate predictions on the testing data using the trained model\n",
    "preds_linear_reg = linear_reg.predict(x_test)\n",
    "\n",
    "# Get the R^2\n",
    "perf_linear_reg = linear_reg.score(x_test, y_test)\n",
    "\n",
    "# Plot the predicted and actual values on a scatter plot\n",
    "plt.scatter(preds_linear_reg, y_test)\n",
    "plt.title(\"Linear Regression: R^2 = \" + str(round(perf_linear_reg, 2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge/Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ridge/lasso regression models\n",
    "# alpha: penalty factor\n",
    "penalty = 1\n",
    "ridge, lasso = linear_model.Ridge(alpha = penalty), linear_model.Lasso(alpha = penalty)\n",
    "\n",
    "# Fit the models to the training data\n",
    "ridge.fit(x_train, y_train)\n",
    "lasso.fit(x_train, y_train)\n",
    "\n",
    "# Generate predictions on the testing data using the trained models\n",
    "preds_ridge, preds_lasso = ridge.predict(x_test), lasso.predict(x_test)\n",
    "\n",
    "# Get the R^2\n",
    "perf_ridge, perf_lasso = ridge.score(x_test, y_test), lasso.score(x_test, y_test)\n",
    "\n",
    "# Plot the predicted and actual values on a scatter plot\n",
    "plt.scatter(preds_ridge, y_test)\n",
    "plt.title(\"Ridge Regression: R^2 = \" + str(round(perf_ridge, 2)))\n",
    "plt.show()\n",
    "plt.scatter(preds_lasso, y_test)\n",
    "plt.title(\"Lasso Regression: R^2 = \" + str(round(perf_lasso, 2)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
